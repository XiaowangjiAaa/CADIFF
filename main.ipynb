{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ISIC_aug_dataset\n",
    "# from torchmetrics import Dice\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import torch.nn.functional as f\n",
    "from monai.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from improved_diffusion.ss_unet import UNetModel_WithSSF\n",
    "from improved_diffusion.script_util import create_gaussian_diffusion\n",
    "from improved_diffusion.resample import UniformSampler\n",
    "from gan import NLayerDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "batch_size = 8\n",
    "epochs = 100\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = \"d:\\DATA\\ISIC2016\\ISIC_augmentation\"\n",
    "save_path = \"./final_result\"\n",
    "dis_save_path = \"./final_result/dis\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(dis_save_path):\n",
    "    os.makedirs(dis_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet hyper parameterts\n",
    "model_channnels = 128\n",
    "in_channels = 4 # 1+3\n",
    "out_channels = 1\n",
    "num_res_blocks = 1\n",
    "attn_resolutions = [] # if use, default is [16]\n",
    "dropout = 0.0\n",
    "channel_mult = (1, 1, 2, 2, 4, 4) if image_size == 256 else None\n",
    "dims = 2\n",
    "num_classes = None\n",
    "num_heads = 4 # not used in model\n",
    "num_heads_upsample = -1 # not used in model\n",
    "use_checkpoint = False\n",
    "use_scale_shift_norm = False\n",
    "\n",
    "num_train_D = 2 # set this\n",
    "num_train_D += 1\n",
    "time_adv_thr = 100\n",
    "loss_adv_weight = 0.5\n",
    "\n",
    "dicefunc = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion hyper parameters\n",
    "steps = 1000\n",
    "learn_sigma = False\n",
    "predict_xstart = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_gan_loss(dis, pred, target, timesteps, mode:str):\n",
    "    assert mode in ['train_G','train_D']\n",
    "    B = pred.shape[0]\n",
    "    weights = torch.where(timesteps < time_adv_thr, 1., 0.)\n",
    "    sum = weights.sum()\n",
    "    if sum > 0:\n",
    "        while len(weights.shape) < len(pred.shape):\n",
    "            weights.unsqueeze_(-1)\n",
    "        idx = torch.where(weights > 0)\n",
    "        idx=idx[0]\n",
    "        pred = torch.index_select(pred, 0, idx)\n",
    "        target = torch.index_select(target, 0, idx)\n",
    "        D = dis\n",
    "        if mode == 'train_G':\n",
    "            output_pred = D(pred)\n",
    "            loss_adv = f.mse_loss(output_pred, torch.ones_like((output_pred)))\n",
    "        if mode == 'train_D':\n",
    "            output_pred = D(pred.detach())\n",
    "            output_target = D(target.float())\n",
    "            loss_adv = f.mse_loss(output_target, torch.ones_like((output_target))) \\\n",
    "                + f.mse_loss(output_pred, torch.zeros_like(output_pred))\n",
    "        return loss_adv * 0.5\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diff_UNet = UNetModel_WithSSF(model_channels=model_channnels, in_channels=in_channels, out_channels=out_channels, channel_mult=channel_mult, num_res_blocks=num_res_blocks, attention_resolutions=attn_resolutions, dropout = dropout, dims=dims, num_classes=num_classes, num_heads=num_heads, num_heads_upsample=num_heads_upsample, use_checkpoint=use_checkpoint, use_scale_shift_norm=use_scale_shift_norm);\n",
    "Diff_UNet.load_resunet(if_pre=False, in_channels=3);\n",
    "Diff_UNet.to(DEVICE);\n",
    "discriminator = NLayerDiscriminator(input_nc=1).to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "if resume:\n",
    "    state_dict = torch.load(os.path.join(save_path, \"diff_unet_v1_withgan_withss.pt\"))\n",
    "    Diff_UNet.load_state_dict(state_dict)\n",
    "    dis_state_dict = torch.load(os.path.join(dis_save_path, \"dis.pt\"))\n",
    "    discriminator.load_state_dict(dis_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = create_gaussian_diffusion(steps=1000, learn_sigma=False, predict_xstart=False)\n",
    "sampler = UniformSampler(diffusion)\n",
    "\n",
    "writer=SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isic_train = ISIC_aug_dataset(path = path, type = 'train', image_size=256)\n",
    "# isic_test = ISIC_aug_dataset(path = path, type = 'test', image_size=256)\n",
    "\n",
    "train_loader = DataLoader(isic_train, batch_size=8, shuffle=True)\n",
    "# test_loader = DataLoader(isic_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_seg = torch.optim.AdamW(Diff_UNet.parameters(), lr = 1e-4, betas=(0.5,0.999))\n",
    "opt_d = torch.optim.AdamW(discriminator.parameters(), lr = 1e-4, betas=(0.,0.999))\n",
    "lr_schedular_seg=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt_seg, T_0=7, T_mult=2)\n",
    "# lr_schedular_d=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt_d, T_0=7, T_mult=2)\n",
    "history={'loss_diff':[], 'loss_D':[], 'loss_G':[], 'loss_dice':[], 'loss_mse':[]}\n",
    "\n",
    "lossfunc1 = nn.MSELoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial for train\n",
    "trainstep=(len(train_loader.dataset)//batch_size)+1 \n",
    "outtertqdm=tqdm(range(epochs))\n",
    "best_loss=100\n",
    "mode_num = 0\n",
    "mode = ['train_G', 'train_D']\n",
    "\n",
    "for epoch in outtertqdm:\n",
    "    # initial for each epoch\n",
    "    innertqdm=tqdm(range(trainstep),leave=False)\n",
    "    dataiter=iter(train_loader)\n",
    "    Diff_UNet.train()\n",
    "    totalLoss_diff = 0\n",
    "    totalLoss_gan_D = 0\n",
    "    totalLoss_gan_G = 0\n",
    "    totalLoss_dice = 0\n",
    "    totalLoss_mse = 0\n",
    "    step = 0\n",
    "    step_D = 0\n",
    "    step_G = 0\n",
    "    \n",
    "    \n",
    "    for _ in innertqdm:\n",
    "        step += 1\n",
    "        mode_num += 1\n",
    "        # initial for each step\n",
    "        (img,real_mask)=next(dataiter)\n",
    "        (img,real_mask)=(img.to(DEVICE),real_mask.to(DEVICE))\n",
    "        t, weights = sampler.sample(img.shape[0], DEVICE)\n",
    "        opt_seg.zero_grad()\n",
    "        opt_d.zero_grad()\n",
    "\n",
    "        #####Loss#####\n",
    "        noise = torch.randn_like(real_mask)\n",
    "        x_t = diffusion.q_sample(real_mask, t, noise=noise)\n",
    "        i_t = torch.cat([x_t, img], dim=1)\n",
    "        pred = Diff_UNet(i_t, diffusion._scale_timesteps(t), img)\n",
    "        pred_xstart = diffusion._predict_xstart_from_eps(x_t, t, pred)\n",
    "        loss = lossfunc1(pred, noise)\n",
    "        predxstart_clip = torch.clamp(pred_xstart, 0, 1)\n",
    "        loss_dice = dicefunc(predxstart_clip, real_mask)\n",
    "        loss_mse = f.mse_loss(pred_xstart, real_mask)\n",
    "\n",
    "        totalLoss_dice += loss_dice\n",
    "        totalLoss_mse += loss_mse\n",
    "        totalLoss_diff += loss\n",
    "        ##### GAN Loss #####\n",
    "        gan_loss = ls_gan_loss(discriminator, pred_xstart, real_mask, t, mode[1 if mode_num % num_train_D else 0])\n",
    "\n",
    "        if gan_loss > 0:\n",
    "            if mode_num % num_train_D :\n",
    "                totalLoss_gan_D += gan_loss\n",
    "                writer.add_scalar('loss_D',float(gan_loss.cpu().detach().numpy()))\n",
    "                step_D += 1\n",
    "            else:\n",
    "                totalLoss_gan_G += gan_loss\n",
    "                writer.add_scalar('loss_G',float(gan_loss.cpu().detach().numpy()))\n",
    "                step_G += 1\n",
    "\n",
    "        # update\n",
    "        total_loss = loss + loss_dice + gan_loss*loss_adv_weight + loss_mse\n",
    "        total_loss.backward()\n",
    "        \n",
    "        opt_seg.step()\n",
    "        if mode[1 if mode_num % num_train_D else 0] == 'train_D':\n",
    "            opt_d.step()\n",
    "            \n",
    "        innertqdm.set_postfix({'step': step + 1, 'loss': loss.cpu().detach().numpy().item()})\n",
    "        writer.add_scalar('loss_diff',float(loss.cpu().detach().numpy()))\n",
    "        writer.add_scalar('loss_dice',float(loss_dice.cpu().detach().numpy()))\n",
    "        writer.add_scalar('loss_mse',float(loss_mse.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "    avgLoss_diff=totalLoss_diff.cpu().detach().numpy()/step\n",
    "    avgLoss_dice=totalLoss_dice.cpu().detach().numpy()/step\n",
    "    avgLoss_mse=totalLoss_mse.cpu().detach().numpy()/step\n",
    "    avgLoss_G=totalLoss_gan_G.cpu().detach().numpy()/step_G\n",
    "    avgLoss_D=totalLoss_gan_D.cpu().detach().numpy()/step_D\n",
    "\n",
    "    history['loss_diff'].append(avgLoss_diff)\n",
    "    history['loss_G'].append(avgLoss_G)\n",
    "    history['loss_D'].append(avgLoss_D)\n",
    "    history['loss_dice'].append(avgLoss_dice)\n",
    "    history['loss_mse'].append(avgLoss_mse)\n",
    "\n",
    "    outtertqdm.set_postfix({'Epoch': epoch+1, 'Loss':avgLoss_diff.item()})\n",
    "\n",
    "    if best_loss>avgLoss_diff:\n",
    "        best_loss=avgLoss_diff\n",
    "        torch.save(Diff_UNet.state_dict(), os.path.join(save_path, 'diff_unet_v1_withgan_withss_best.pt'))\n",
    "    torch.save(Diff_UNet.state_dict(), os.path.join(save_path, 'diff_unet_v1_withgan_withss.pt'))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(save_path, 'dis','dis.pt'))\n",
    "\n",
    "    lr_schedular_seg.step()\n",
    "    # lr_schedular_d.step()\n",
    "\n",
    "with open(os.path.join(save_path, 'history_resunet.pkl'),'wb') as f:\n",
    "    pickle.dump(history,f)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
